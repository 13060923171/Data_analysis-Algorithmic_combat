{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "befcb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "#设置seaborn的主题样式为白色网格,另外还有darkgrid(默认)/dark/white/ticks.\n",
    "sns.set_style(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79852d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.read_csv('xxxx.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31f301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据标准化 \n",
    "from sklearn import preprocessing \n",
    "x_scaled = preprocessing.scale(X)\n",
    "x_scaled = pd.DataFrame(x_scaled,columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a36f3534",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Kmeans 寻找最优解"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0944b15",
   "metadata": {},
   "source": [
    "#当K值设置不同时，聚类效果会存在较大的差别，因此在实际应用中需要依据聚类效果，对K值进行优化。执行如下代码，通过轮廓系数(Calinski-Harabasz分数值)对不同的K值进行评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a672395c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对不同的K值进行计算，筛选最优的K值\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "#kMeans 算法实例化，将其设置为K=range(2,14)\n",
    "d = {}\n",
    "fig_reduced_data = plt.figure(figsize=(12,12))\n",
    "for k in range(2,14):\n",
    "    est = KMeans(n_clusters=k,random_state=111)\n",
    "    #作用到标准化后的数据集上\n",
    "    y_pred = est.fit_predict(x_scaled)\n",
    "    #距离越来越小，效果越来越好\n",
    "    score = metrics.calinski_harabasz_score(x_scaled,y_pred)\n",
    "    d.update({k:score})\n",
    "    print('calinski_harabasz_score with k={0} is {1}'.format(k,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e210b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for  k,score in d.items():\n",
    "    x.append(k)\n",
    "    y.append(score)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('calinski_harabasz_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69393ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据降维\n",
    "#使用PCA进行数据降维\n",
    "from sklearn.decomposition import PCA\n",
    "#此外的主成分维度我们人为设定为3，对于属性较少的数据集，属于常规会选择的维度数，后面也会看\n",
    "pca = PCA(n_components=3)\n",
    "x_pca = pca.fit_transform(x_scaled)\n",
    "x_pca_frame = pd.DataFrame(x_pca,columns=['pca_1','pca_2','pca_3'])\n",
    "x_pca_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c65e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#降维指标与原始指标的关联关系\n",
    "#这三个指标与原始字段的系数可以通过pca的components_属性获取\n",
    "#绘制主成分与原数据的热图，更直观的反应他们之间的关系\n",
    "plt.matshow(pac.components_,cmap='plasma')\n",
    "plt.xticks(range(7),X.columns,fontproperties = 'SimHei')\n",
    "#y轴显示三个主成分\n",
    "plt.yticks([0,1,2],['pca_1','pca_2','pca_3'])\n",
    "plt.colorbar()\n",
    "#显示图像\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85d9d60",
   "metadata": {},
   "source": [
    "从热力图中，我们可以看出每个原始数据和主成分之间的关系，暖色代表正相关，冷色代表负相关，数值的绝对值越大代表相关性越强。至此，我们可以得出，三个主成分基本覆盖了所有的原数据。下面我们再次进行K值寻优操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42554649",
   "metadata": {},
   "outputs": [],
   "source": [
    "#对不同的K值进行计算，筛选最优的K值\n",
    "\n",
    "###这里注意 是采用降维之后的数据，重新进行K值的判断！！！！！！！\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "#kMeans 算法实例化，将其设置为K=range(2,14)\n",
    "d = {}\n",
    "fig_reduced_data = plt.figure(figsize=(12,12))\n",
    "for k in range(2,14):\n",
    "    est = KMeans(n_clusters=k,random_state=111)\n",
    "    #作用到标准化后的数据集上\n",
    "    y_pred = est.fit_predict(x_scaled)\n",
    "    #距离越来越小，效果越来越好\n",
    "    score = metrics.calinski_harabasz_score(x_pca_frame,y_pred)\n",
    "    d.update({k:score})\n",
    "    print('calinski_harabasz_score with k={0} is {1}'.format(k,score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35ac9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for  k,score in d.items():\n",
    "    x.append(k)\n",
    "    y.append(score)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.xlabel('k value')\n",
    "plt.ylabel('calinski_harabasz_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bfcac6",
   "metadata": {},
   "source": [
    "可以发现，进行PCA数据降维之后，聚类效果更好；另外从聚类结果图片可以看出，对于这个实验，当K为2的时候，聚类效果较好\n",
    "这里有一点是需要注意的，当PCA进行降维的时候，如果N =2 或者 N =3 ，他们最后的结果一样，都是K为2效果最佳，这时候就是看他们的score\n",
    "他们的score越小，效果则越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe1456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-means聚类建模\n",
    "#运行Kmean聚类算法\n",
    "from sklearn.cluster import KMeans\n",
    "#此处指定K= 12\n",
    "est = KMeans(n_clusters=12)\n",
    "est.fit(x_pca)\n",
    "kmeans_clustering_labels = pd.DataFrame(est.labels_,columns=['cluster'])\n",
    "#将聚类结果与降维特征数据进行拼接\n",
    "x_pca_frame = pd.concat([x_pca_frame,kmeans_clustering_labels],axis= 1)\n",
    "x_pca_frame.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab4f666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除噪音点\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x.index = x_pca_frame.index\n",
    "x_full = pd.concat([x,x_pca_frame],axis=1)\n",
    "grouped = x_full.groupby('cluster')\n",
    "result_data = pd.DataFrame()\n",
    "for name,group in grouped:\n",
    "    print(name,group['pca_1'].count())\n",
    "    desp = group[['pca_1','pca_2','pca_3']].describe()\n",
    "    for att in ['pca_1','pca_2','pca_3']:\n",
    "        lower25 = desp.loc['25%',att]\n",
    "        upper75 = desp.loc['75%',att]\n",
    "        IQR = upper75 - lower25\n",
    "        min_value = lower25 - 1.5 * IQR\n",
    "        max_value = upper75 + 1.5 * IQR\n",
    "        group = group[(group[att] > min_value) & (group[att] < max_value)]\n",
    "    result_data = pd.concat([result_data,group],axis=0)\n",
    "    print(name,group['pca_1'].count())\n",
    "#分别列出 筛选前后每个特征的数量\n",
    "print('remanin sample:',result_data['pca_1'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc189dd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tools/features_logits/lane_embedding_feats.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\V_YUHA~1\\AppData\\Local\\Temp/ipykernel_24408/259294026.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0membedding_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./tools/features_logits/lane_embedding_feats.npy\"\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 导入数据，数据格式为（samples，）\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mret\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDBSCAN_Cluster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_features\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# 进行 DBSCAN聚类\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tools/features_logits/lane_embedding_feats.npy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glog as log\n",
    " \n",
    "from sklearn.cluster import DBSCAN  # 进行DBSCAN聚类\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score ,calinski_harabasz_score,davies_bouldin_score # 计算 轮廓系数，CH 指标，DBI \n",
    " \n",
    " \n",
    "# 定义一个进行DBSCAN的函数\n",
    "def DBSCAN_Cluster(embedding_image_feats):\n",
    "    \"\"\"\n",
    "    dbscan cluster\n",
    "    :param embedding_image_feats:  # 比如形状是（9434,4）表示9434个像素点\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    db = DBSCAN(eps=0.35, min_samples=600)\n",
    "    try:\n",
    "        features = StandardScaler().fit_transform(embedding_image_feats)  # 将特征进行归一化\n",
    "        db.fit(features)\n",
    "    except Exception as err:\n",
    "        log.error(err)   \n",
    "        ret = {\n",
    "            'origin_features': None,\n",
    "            'cluster_nums': 0,\n",
    "            'db_labels': None,\n",
    "            'cluster_center': None\n",
    "            }\n",
    "        return ret\n",
    " \n",
    "    db_labels = db.labels_                  # 获取聚类之后没一个样本的类别标签\n",
    "    unique_labels = np.unique(db_labels)    # 获取唯一的类别\n",
    " \n",
    "    num_clusters = len(unique_labels)\n",
    "    cluster_centers = db.components_\n",
    " \n",
    "    ret = {\n",
    "            'origin_features': features,      #(9434,4)\n",
    "            'cluster_nums': num_clusters,     # 5  它是一个标量，表示5类，包含背景\n",
    "            'db_labels': db_labels,           #(9434,)\n",
    "            'unique_labels': unique_labels,   #(5,)\n",
    "            'cluster_center': cluster_centers #(6425,4)\n",
    "        }\n",
    " \n",
    "    return ret\n",
    " \n",
    "# 画出聚类之后的结果\n",
    "def plot_dbscan_result(features,db_labels,unique_labels,num_clusters):\n",
    "    colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels)))\n",
    "    for k, color in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "           color = 'k'  # 黑色的，这代表噪声点\n",
    " \n",
    "        index=np.where(db_labels==k)   #  获取每一个类别的索引位置\n",
    "        x=features[index]\n",
    " \n",
    "        plt.plot(x[:, 0], x[:, 1], 'o', markerfacecolor=color,markeredgecolor='k', markersize=6)\n",
    " \n",
    "    plt.title('Estimated number of clusters: %d' % num_clusters)\n",
    "    plt.show()\n",
    " \n",
    " \n",
    "if __name__=='__main__':\n",
    "    embedding_features=np.load(\"./tools/features_logits/lane_embedding_feats.npy\")  # 导入数据，数据格式为（samples，）\n",
    " \n",
    "    ret=DBSCAN_Cluster(embedding_features)  # 进行 DBSCAN聚类\n",
    " \n",
    "    plot_dbscan_result(ret['origin_features'],ret['db_labels'],ret['unique_labels'],ret['cluster_nums']) # 展示聚类之后的结果\n",
    " \n",
    "    \n",
    "    s1=silhouette_score(embedding_features, ret['db_labels'], metric='euclidean') # 计算轮廓系数\n",
    "    s2=calinski_harabasz_score(embedding_features,ret['db_labels']) # 计算CH score\n",
    "    s3=davies_bouldin_score(embedding_features,ret['db_labels'])    # 计算 DBI\n",
    " \n",
    "    print(s1)\n",
    "    print(s2)\n",
    "    print(s3)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772b4ea2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
