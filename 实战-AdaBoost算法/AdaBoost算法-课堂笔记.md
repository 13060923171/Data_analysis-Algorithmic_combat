# AdaBoost算法是属于分类算法中的集成算法

集成算法通常有两种方式：投票选举和再学习

- 投票选举的场景类似专家召集到会议室里面，当做一个决定的时候，让K个专家（K个模型）分别进行分类，然后选择出现次数最多的那个类作为最终的分类结果。
- 再学习相对于把K个专家（K个分类器）进行加权融合，形成一个新的超级专家（强分类器），让这个超级专家做判断



再学习是提升，它的作用是每一次训练的时候都对上一次的训练进行改进提升，在训练的过程中这K个“专家”之间是有依赖性的，当引入第K个“专家（第K个分类器）的时候，实际上是对前K-1个专家的优化。”

投票选举在运作的时候可以并行计算，也就是K个“专家”在做判断的时候是相互独立的，不存在依赖性



## AdaBoost的工作原理

就是对boosting算法的一种实现，就是通过训练多个弱化器，将这些弱化器组合成一个强分类器，也就是三个臭皮匠顶一个诸葛亮的意思

写成公式就是如下：

![image-20210812193909155](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20210812193909155.png)

![image-20210812193938350](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20210812193938350.png)

![image-20210812193952811](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20210812193952811.png)

![image-20210812194008792](https://cdn.jsdelivr.net/gh/13060923171/images@main/img/image-20210812194008792.png)

## 总结

对于AdaBoost算法的原理，其实就是一种集成算法，通过训练不同的弱分类器，将这些弱分类器集成起来形成一个强分类器。在每一轮的训练中都会加入一个新的弱分类器，直到达到足够低的错误率或者达到指定的最大迭代次数为止。实际上每一次迭代都会引入一个新的弱分类器，在这些弱分类器的集合中，不用过于担心弱分类器太弱，实际上它只需要比随机猜测的效果略好一些就好了

