# 数据分析之决策树

## 决策树的工作原理

决策树基本上就是把我们以前的经验总结起来，我给你准备一个打篮球的训练集。如果我们要出门打篮球，一般会根据'天气'、’温度‘、’湿度‘、’刮风‘这几个条件来判断，最后得到的结果：去打篮球？还是不去?

![image-20210627202832359](https://img-blog.csdnimg.cn/img_convert/03384529b3b92889b1e0e447c0942e47.png)

### 一般做决策树会有两个阶段：构造和剪枝

#### 构造

构造的过程就是选择什么属性作为节点的过程

1. 跟节点：就是树的顶端
2. 内部节点：就是树中间的那些节点
3. 叶节点：就是树最底部的节点

#### 剪枝

剪枝就是给决策树瘦身，之所以这么做就是为了防止过拟合

过拟合就是说这个模型训练的太好了，不符合实际情况

欠拟合和过拟合就好比下面的图片，训练结果太好反而在实际应用过程中会导致分类错误

![image-20210627203529917](https://img-blog.csdnimg.cn/img_convert/c584dd0f9b1a96e757c14f1f1ae9fd55.png)

一般导致过拟合的原因是训练集太小了，导致决策树容易在真实数据分类中出现错误



## 用一个实际例子来展示更容易方便大家理解

![image-20210627204332611](https://img-blog.csdnimg.cn/img_convert/14dac0c612dcccdf7723a1f1e062a486.png)

这里采用一个篮球的训练集

在开始先，要提及一下关于**纯度和信息熵**的概念

### 纯度

这个你可以把决策树的构造过程理解成为寻找纯净值划分的过程。数学上，我们可以用纯度来表示，纯度换成一种方式来解释就是让目标变量的分歧最小

假如有3个集合

- 集合1：6次都去打篮球
- 集合2：4次都去打篮球，2次不去打篮球
- 集合3：3次都去打篮球，3次不去打篮球

按照纯度指标来说，集合1>集合2>集合3。因为集合1的分歧最小，集合3的分歧最大

### 信息熵

它表示了信息的不确定度

计算信息熵的公式：

![image-20210627205219848](https://img-blog.csdnimg.cn/img_convert/cad29d859ec33677f18d02ea17eca22e.png)

p(i|t)代表了节点t为分类i的概率，其中log2为取以2为底的对数，当不确定性越大，它所包含的信息量也就越大，信息熵也就越高

假如有2个集合

- 集合1: 5次去打篮球,1次不去打篮球
- 集合2： 3次去打篮球，3次不去打篮球

计算集合1的信息熵就是：

​	Entropy(t) = -(1/6)log2(1/6) - (5/6)log2(5/6) = 0.65

计算集合2的信息熵就是：

​	Entropy(t) = -(3/6)log2(3/6) - (3/6)log2(3/6) = 1

从上面的计算结果中可以看出，信息熵越大，纯度越低。当集合中的所有样本均匀混合时，信息熵最大，纯度最低

## 在构造决策树的时候，会基于纯度来构造。而经典的“不纯度”的指标有三种，分别是信息增益（ID3算法）、信息增益率（C4.5算法）以及基尼指数（Cart算法）

### ID3算法

ID3算法计算的是信息增益，信息增益指的是划分可以带来纯度的提高，信息熵的下降。计算公式为父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，提高每一个子节点的归一化信息熵，既按照每一个子节点在父节点出现的概率，来计算这些子节点的信息熵，公式为：

![image-20210701203806361](https://img-blog.csdnimg.cn/img_convert/d2046f3792036d5dc263d48c36aad701.png)

公式中D是父亲节点，Di是子节点，Gain(D,a)中的a作为D节点的属性选择

例如：

假设天气 = 晴的时候，会有5次去打篮球，5次不去打篮球，其中D1刮风 = 是，有2次打篮球，1次不打篮球，D2 = 否，有3次打篮球，4次不打篮球，那么a节点代表的是天气 = 晴

![image-20210701204216648](https://img-blog.csdnimg.cn/img_convert/d7743e9c652b84a4d93170a9ea9703d1.png)

公式可以写成：

![image-20210701204244504](https://img-blog.csdnimg.cn/img_convert/5b6b30d53365544651bd29e0a9f795e3.png)

也就说D节点的信息熵-2个子节点的归一信息熵。2个子节点归一化信息熵 = 3/10的D1信息熵 + 7/10的D2的信息熵

基于ID3的算法规则，完整的计算出我们的训练集，训练集一共有7条数据，3个打篮球，4个不去打篮球，根节点的信息熵是：

![image-20210701205908502](https://img-blog.csdnimg.cn/img_convert/f7ae211f18b464187b2a3884f5bab04e.png)

如果将天气作为属性划分，会有三个叶子节点，D1 D2 D3分别对应的是晴天，阴天，小雨，用+号代表打篮球，-号代表不去打篮球，公式如下：

![image-20210701210930288](https://img-blog.csdnimg.cn/img_convert/fcb6bd58918966fb8783c8e61e6cd7ff.png)

### 因为我们用ID3中的信息增益来构造决策树，所以要计算每个节点的信息增益

天气作为属性节点的信息增益为，Gain（D，天气）  = 0.985-0.965 = 0.020

同理可以计算其他属性作为根节点的信息增益，如下

Gain(D,温度) = 0.128

Gain(D,湿度) =  0.020

Gain(D,刮风) = 0.020

从上面看温度作为属性的信息增益最大，因为ID3就是要将信息增益最大的节点作为父节点，这样就可以得到纯度高的决策树，所以我们将温度作为根节点。如下：

![image-20210701211636087](https://img-blog.csdnimg.cn/img_convert/9b9b7504233b5a58289e12d3a5c26765.png)

然后再对第一个叶节点进行分裂，计算其不同属性（天气，温度，刮风）作为节点的信息增益，如图：

![image-20210701211808939](https://img-blog.csdnimg.cn/img_convert/f04eeb4ed5f63c8567ad1ad510f028f8.png)

这样就可以得到一颗决策树，ID3的算法相对比较简单，可解释性强，同样也存在缺陷，ID3倾向选择取值比较多的属性。如果把”编号“作为一个属性，那么”编号“会被选为最优属性，但实际上”编号“是无关属性的，它对”打篮球“分类并没有太大作用。

所以ID3的缺陷就是，容易对某个无意义的属性进行误判

## 因此C4.5算法就是在ID3的基础上进行改良

**改良的方向：**

### 1、采用信息增益率

C4.5采用信息增益率的方式来选择属性。信息增益率 = 信息增益 /属性熵

当属性有很多值的时候，相当于被划分成许多份，虽然信息增益变大了，但对于C4.5来说，属性熵也会变大，所以整体的信息增益率并不大

### 2、采用悲观剪枝

在C4.5中，会对决策树构造进行悲观剪枝（PER），这样可以提升决策树的泛化能力

悲观剪枝是后剪枝技术的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后，这个节点的分类错误率来决定是否对其进行剪枝，这种剪枝方法不再需要一个单独的测试数据集

### 3、离散化处理连续属性

C4.5可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高”，“中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能，该怎么选择这个阈值，**C4.5选择具有最高信息增益的划分所对应的阈值**

### 4、处理缺失值

针对数据集不完整，C4.5也可以进行处理

假如我们得到的是如下的数据，你会发现这个数据中存在两点，第一个问题是，数据集中存在数值缺少的情况，如何进行属性选择？第二个问题是，假设已经做了属性划分，但是样本在这个属性上有缺少值，该如何对样本进行划分？

数据集如下：

![image-20210702210000317](https://img-blog.csdnimg.cn/img_convert/38593b253e6b54aaa514cf585903ef07.png)

我们不考虑缺少的数值，可以得到温度D={2-，3+，4+，5-，6+7-}。温度 = 高：D1 = {2-,3+,4+};温度 = 中：D2 = {6+,7-};温度 = 低: D3={5-}。这里+号代表打篮球，-号代表不打篮球。比如ID=2时，决策是不打篮球，我们可以记录为2-

针对将属性选择为温度的信息增为：

Gain(D,温度) = Ent(D)-0.792 = 1.0-0.792 = 0.208

属性熵 = 1.459，信息增益率Gain_ratio(D,温度) = 0.208/1.459 = 0.1426

D的样本个数为6，而D的样本个数为7，所以所占权重比例为6/7，所以Gain(D,温度)所占权重比例为6/7，所以：

Gain_ratio(D,温度) = 6/7*0.1426 = 0.122

这样即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择

## 总结

ID3算法的优点是方法简单，缺点是对噪声敏感，容易进行误判

C4.5在ID3的基础上，用信息增益率代替信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝，处理连续数值以及数值缺少等情况，但是由于C4.5需要对数据集进行多次扫描，算法效率较低

即使在温度属性的数值有缺失的情况下，我们依然可以计算信息增益，并对属性进行选择

## 总结

ID3算法的优点是方法简单，缺点是对噪声敏感，容易进行误判

C4.5在ID3的基础上，用信息增益率代替信息增益，解决了噪声敏感的问题，并且可以对构造树进行剪枝，处理连续数值以及数值缺少等情况，但是由于C4.5需要对数据集进行多次扫描，算法效率较低

