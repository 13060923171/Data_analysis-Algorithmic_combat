# KNN的工作原理

通俗易懂的来说就是“近朱者赤近墨者黑”，就是计算出哪些是一类的，将它们区分出来

计算的过程分三步走：

- 计算待分类物体与其他物体之间的距离
- 统计距离最近的K个邻居
- 对于K个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类

## K值如何选择

1、如果K值比较小，就相当于未分类物体与它的邻居非常接近才行。这样产生的一个问题就是，如果邻居点是个噪声点，那么未分类物体的分类也会产生误差，这样KNN分类就会产生过拟合

2、如果K值比较大，相当于距离过远的点也会对未知物体的分类产生影响，虽然这种情况的好处是鲁棒性强，但是不足也很明显，会产生欠拟合情况，也就是没有把未分类物体真正分类出来

所以K值都是需要我们去实践出来的，通过采用交叉验证的方式选取K值，交叉验证的思路就是把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性，在knn算法中，我们一般会把K值选取在较小的范围内，同时在验证集上准确率最高的那一个最终确定作为K值

## 距离的计算方式

计算距离有五种方式：

1. 欧氏距离
2. 曼哈顿距离
3. 闵可夫斯基距离
4. 切比雪夫距离
5. 余弦距离

前面三种是KNN最常用的距离

![image-20210806172334349](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20210806172334.png)

![image-20210806172349962](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20210806172350.png)

![image-20210806172402105](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20210806172402.png)

![image-20210806172411341](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20210806172411.png)

## KD树

KNN的计算过程是大量样本点之间的距离，为了减少计算距离次数，提升KNN的搜索效率，KD树就出生了，KD树是对数据点在K纬空间中划分的一种数据结构，在KD树的构造中，每个节点都是K纬数值点的二叉树，既然是二叉树，就可以采用二叉树的增删改查的操作，提高搜索效率

## KNN做回归

![image-20210806172706899](https://gitee.com/mqsnq30/gitee-table/raw/master/img/20210806172707.png)

## 总结

KNN的算法原理和工作流程是怎么样的，KNN中的K值是如何选择的

- KNN算法的核心思想是如果一个样本在特征空间中的K个最相邻的样本中的大多数属于某一个类别，则该样本也属于这个类别，并具有这个类别上样本的特性。
- 整个计算过程为三步
  - 计算待分类物体与其他物体之间的距离
  - 统计距离最近的K个邻居
  - 对应K个最近的邻居，它们属于哪个分类最多，待分类物体就属于哪一类
  - 我们一般采用交叉验证的方式选取K值
- 交叉验证的思路就是，把样本集中的大部分样本作为训练集，剩余的小部分样本用于预测，来验证分类模型的准确性，准确率最高的那一个最终确定作为K值